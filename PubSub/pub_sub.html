<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Pub-Sub Using kafka | Manubhav Jain</title>
    <script src="../main.js"> </script>
    <link rel="stylesheet" href="../blog_style.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
</head>
<body>
    <header>
        <nav>
            <a href="../index.html" class="logo">MJ</a>
            <div class="hamburger" onclick="toggleNav()">&#9776;</div>
            <div class="nav-links" id="navLinks">
                <a href="../index.html#about">About</a>
                <a href="../index.html#experience">Experience</a>
                <a href="../index.html#skills">Skills</a>
                <a href="../index.html#projects">Projects</a>
                <a href="../index.html#blog">Blog</a>
                <a href="../index.html#research">Research</a>
                <a href="../index.html#contact">Contact</a>
            </div>
        </nav>
    </header>

    <section class="blog-header">
        <img src="kafka.png" alt="Dashboard" class="blog-image" style="width: 90%; max-width: 50rem;">
        <div class="blog-header-content">
            <p class="blog-category">Distributed Systems</p>
            <h1 class="blog-title">Pub-Sub Using kafka</h1>
            <div class="blog-meta">
                <span><i class="far fa-calendar-alt"></i> April 24, 2025</span>
                <span><i class="far fa-clock"></i> 10 min read</span>
                <span><i class="fas fa-tag"></i> Kafka, Pub-Sub</span>
            </div>
        </div>
    </section>

    <div class="blog-content">
        <h2>What is Pub-Sub?</h2>
        <p>
            Pub/Sub messaging enables you to create an event-driven systems consisting of event producers and consumers, called publishers and subscribers. Publishers asynchronously broadcast the events to communicate, rather than by synchronous remote procedure calls (RPCs), in which publishers must wait for subscribers to receive the data. However, the asynchronous integration in Pub/Sub increases the flexibility and robustness of the overall system.
        </p>
        <p align="center">
            <img src="https://miro.medium.com/max/1400/1*iUWxneAQ_kozzLPkFsrakw.png">
          
          </p>

          <h3>Apache Kafka</h3>
        <p>
            Apache Kafka is the open-source stream processing software platform, which is written in Scala and Java. Apache Kafka is a fast, scalable, fault-tolerant messaging system which enables communication between producers and consumers using message-based topics. Kakfa is highy resilient of node failures and support auto-recovery. This makes Apache Kafka ideal for communication and integration between components of large-scale data systems in real-world data systems.
        </p>
        
        <h3>Brokers</h3>
        
        <p>
            Kafka consists of a network of machines called brokers. these may not be separate physical servers but containers running on pods running on virtualized servers running on actual processors in a physical datacenter somewhere. However they are deployed, they are independent machines each running the Kafka broker process. Since, there can be multiple brokers there needs to be a central service which keeps track of all the states and configurations of all kafka brokers, hence we introduce zookeeper. 
        </p>
        
        <h3>Zookeeper</h3>
        <p>
            Kafka and ZooKeeper work in conjunction to form a complete Kafka Cluster. With ZooKeeper providing the distributed clustering services, and Kafka handling the actual data streams and connectivity to clients. At a detailed level, ZooKeeper handles the leadership election of Kafka brokers and manages service discovery as well as cluster topology so each broker knows when brokers have entered or exited the cluster, when a broker dies and who the preferred leader node is for a given topic/partition pair. It also tracks when topics are created or deleted from the cluster and maintains a topic list.
        <p>
            For several years, the Apache Kafka community has been developing a new way to run Apache Kafka with self-managed metadata. Kafka 3.3 introduces a new mode, called KRaft mode, which improves partition scalability and resiliency while simplifying deployments of Apache Kafka. It also eliminates the need to run an Apache ZooKeeper cluster alongside every Apache Kafka cluster. Although KRaft marks as production ready for only new clusters, there are some features that are only supported by Apache Zookeeper.
        </p>

        <h3>Kafka logging Mechanism</h3>
        <p>
            Kafka is based on commit log, which means Kafka stores a log of records and it will keep a track of what’s happening. This log storage mechanism is similar with common RDBMS uses. The mechanisms is  more like a queue where you always append a new data into the tail. It may seems simple, but Kafka can maintain the records into several partitions with same topic. A topic is a category or feed name to which records are published. So, rather than just write into one queue, Kafka can writes into several queue with same topic name.
        <p>
        
        <h2>Basic Pub-Sub Application using kafka and golang</h2>    
        <h3>Setup Kafka using Docker</h3>
         
        <p>Here is the docker-compose.yaml used to setup kafka:</p>
        <div class="code-block">
            version: '2'
            services:
                zookeeper:
                    image: confluentinc/cp-zookeeper:latest
                    environment:
                        ZOOKEEPER_CLIENT_PORT: 2181
                        ZOOKEEPER_TICK_TIME: 2000
                    ports:
                         - 22181:2181
            
                kafka:
                    image: confluentinc/cp-kafka:latest
                    depends_on:
                        - zookeeper
                    ports:
                        - 9093:9093
                    environment:
                        KAFKA_BROKER_ID: 1
                        KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
                        KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:9093
                        KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
                        KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
                        KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
                        KAFKA_CREATE_TOPICS: my-kafka-topic
        </div>        
        <p>
            In this docker-compose file kafka image used is:
            <code>confluentinc/cp-kafka:latest</code>
            And the zookeeper image used is 
            <code>confluentinc/cp-zookeeper:latest</code>
            Since it is a basic pubsub we would be using only one broker instance
            and the topics created is: <code>my-kafka-topic</code>
            
            Now since the kafka image has been set up now we need to define publishers and consumers.
            
            This will be done using go-library:
            <code>github.com/segmentio/kafka-go</code>
        </p>

        <p>The main part of publishing message into Kafka is you must create the connection into Kafka then you can produce a message to Kafka using established connection. For this we will be creating kakfa-writer function in which we define all the kafka configurations and establish connections to kafka.</p>
        
        <div class="code-block">
            func getKafkaWriter(kafkaURL, topic string) *kafka.Writer {
                return &kafka.Writer{
                    Addr:     kafka.TCP(kafkaURL),
                    Topic:    topic,
                    Balancer: &kafka.LeastBytes{},
                }
            }
        </div>

        <p>
            After the kafka connection is established, the <code>kafkaWriter</code> object can be used to broadcast data from publisher.
            Below is the basic golang producer Method we have implemented:
        </p>
        <div class="code-block">
            func producer(kafkaWriter *kafka.Writer, ctx context.Context) {
                var body string
                for {
                    fmt.Scanf("%v", &body)
                    msg := kafka.Message{
                    Key:   []byte(fmt.Sprintf("address-%s", broker1Address)),
                    Value: []byte(body),
                    }
                    err := kafkaWriter.WriteMessages(ctx, msg)
                    if err != nil {
                        log.Fatalln(err)
                    }
                }
            }
        </div>

        <p>
            In this we have created a message object which have a key value structure and we are sending the message via kafkaWriter object we have implement earlier.

            Once the  message is published the consumer who is subscribed to same topic to which kafka broker is configured can recieved it.
            Below is the basic golang consumer method we have implemented:           
        </p>

        <div class="code-block">
            func consume(ctx context.Context, group string) {
                r := kafka.NewReader(kafka.ReaderConfig{
                    Brokers: []string{broker1Address},
                    Topic:   topic,
                    GroupID: group,
                })
        
                for {
                    msg, err := r.ReadMessage(ctx)
                    if err != nil {
                        panic("could not read message " + err.Error())
                    }
                    fmt.Printf("%v received: %v", group, string(msg.Value))
                    fmt.Println()
                    }
            }
        </div>

        <p>
            Here we have implemented a kafka reader object with same configuration and topics as kafka broker to establish connection with kafka and recieve messages.

            Below is the <code>main()</code> function for the produce and consumer
            
        </p>

        <div class="code-block">
            func main() {
                ctx := context.Background()
                go producer(getKafkaWriter(broker1Address, topic), ctx)
                go consume(ctx, "grp1")
                time.Sleep(time.Minute * 2)
            }              
        </div>

        <p>
            Below is the output of above-mentioned code
            <code>
            Hello
            grp1 received: Hello
            World
            grp1 received: World
            
            Process finished with the exit code 0
            </code>
        </p>



<h2>Strengths of Kafka</h2>
<h3>a. High-throughput</h3>
<p>
    Without having not so large hardware, Kafka is capable of handling high-velocity and high-volume data. Also, able to support message throughput of thousands of messages per second.
</p>
<h3>b. Low Latency</h3>
<p>
    It is capable of handling these messages with the very low latency of the range of milliseconds, demanded by most of the new use cases.
</p>
<h3>c. Fault-Tolerant</h3>
<p>
    One of the best advantages is Fault Tolerance. There is an inherent capability in Kafka, to be resistant to node/machine failure within a cluster. 
</p>
<h3>d. Durability</h3>
<p>
    Here, durability refers to the persistence of data/messages on disk. Also, messages replication is one of the reasons behind durability, hence messages are never lost.
</p>
<h3>e. Scalability</h3>
<p>
    Without incurring any downtime on the fly by adding additional nodes, Kafka can be scaled-out. Moreover, inside the Kafka cluster, the message handling is fully transparent and these are seamless.
</p>
<h3>f. Distributed</h3>
<p>
    The distributed architecture of Kafka makes it scalable using capabilities like replication and partitioning.
</p>
<h3>g. Message Broker Capabilities</h3>
<p>
    Kafka tends to work very well as a replacement for a more traditional message broker. Here, a message broker refers to an intermediary program, which translates messages from the formal messaging protocol of the publisher to the formal messaging protocol of the receiver.
</p>
<h3>h. High Concurrency</h3>
<p>
Kafka is able to handle thousands of messages per second and that too in low latency conditions with high throughput. In addition, it permits the reading and writing of messages into it at high concurrency.
</p>
<h3>i. Consumer Friendly</h3>
<p>
It is possible to integrate with the variety of consumers using Kafka. The best part of Kafka is, it can behave or act differently according to the consumer, that it integrates with because each customer has a different ability to handle these messages, coming out of Kafka. Moreover, Kafka can integrate well with a variety of consumers written in a variety of languages.
</p>


<div class="github-link">
    <a href="https://github.com/mannu2411/pub_sub_kafka" target="_blank">
        <i class="fab fa-github"></i> View on GitHub
    </a>
</div>


<div class="blog-tags">
    <span class="blog-tag">WebSockets</span>
    <span class="blog-tag">Kafka</span>
    <span class="blog-tag">Redis</span>
    <span class="blog-tag">Go</span>
    <span class="blog-tag">Distributed Systems</span>
    <span class="blog-tag">Real-time</span>
</div>

<div class="blog-author">
    <a href="../index.html" class="logo">MJ</a>
    <div class="author-content">
        <h3 class="author-name">Manubhav Jain</h3>
        <p class="author-bio">Backend engineer specializing in distributed systems and real-time applications. Currently working on scalable messaging infrastructure and cloud-native applications.</p>
        <div class="author-social">
            <a href="https://github.com/mannu2411"><i class="fab fa-github"></i></a>
            <a href="https://leetcode.com/u/mannu2411/" title="LeetCode"><i class="fas fa-code"></i></a>
            <a href="https://www.linkedin.com/in/manubhav-jain-55382a154/"><i class="fab fa-linkedin"></i></a>
            <a href="#"><i class="fas fa-globe"></i></a>
        </div>
    </div>
</div>

<div class="related-posts">
    <h2 class="related-title">Related Posts</h2>
    <div class="related-grid">
        <div class="related-card">
            <div class="related-image">
                <img src="../Dashboard/dynamic-dashboard.svg" alt="Dynamic dashboard architecture diagram">
            </div>
            <div class="related-content">
                <h3 class="related-card-title">Real time Dashboard using websockets</h3>
                <p class="related-date">March 15, 2025</p>
                <a href="../Dashboard/dashboard.html" class="read-more">
                    Read More <i class="fas fa-arrow-right"></i>
                </a>
            </div>
        </div>
        <div class="related-card">
            <div class="related-image">
                <img src="../Reflect/reflect-in-go.png" alt="Reflection in Golang">
            </div>
            <div class="related-content">
                <h3 class="related-card-title">Reflection in Golang</h3>
                <p class="related-date">January 19, 2025</p>
                <a href="../Reflect/reflect.html" class="read-more">
                    Read More <i class="fas fa-arrow-right"></i>
                </a>
            </div>
        </div>
        <div class="related-card">
            <div class="related-image"> 
                <img src="../Redis PubSub/redis-pub-sub.png" alt="Pub-Sub using Redis">
            </div>
            <div class="related-content">
                <h3 class="related-card-title">Pub-Sub using Redis</h3>
                <p class="related-date">December 10, 2024</p>
                <a href="../Redis PubSub/redis_pub_sub.html" class="read-more">
                    Read More <i class="fas fa-arrow-right"></i>
                </a>
            </div>
        </div>
    </div>
        </div>
</div>

<div class="back-to-portfolio">
    <a href="../index.html" class="button">Back to Portfolio</a>
</div>
</div>

<footer>
<div class="footer-content">
    <div class="footer-links">
        <a href="../index.html#about">About</a>
        <a href="../index.html#experience">Experience</a>
        <a href="../index.html#skills">Skills</a>
        <a href="../index.html#projects">Projects</a>
        <a href="../index.html#blog">Blog</a>
        <a href="../index.html#contact">Contact</a>
    </div>
    <div class="footer-social">
        <a href="#"><i class="fab fa-github"></i></a>
        <a href="#"><i class="fab fa-twitter"></i></a>
        <a href="#"><i class="fab fa-linkedin"></i></a>
        <a href="#"><i class="fab fa-instagram"></i></a>
    </div>
    <p class="footer-text">© 2025 Manubhav Jain. All rights reserved.</p>
</div>
</footer>

<script>
window.addEventListener('scroll', function() {
    const header = document.querySelector('header');
    if (window.scrollY > 50) {
        header.style.padding = '15px 50px';
        header.style.boxShadow = '0 10px 30px -10px rgba(2, 12, 27, 0.7)';
    } else {
        header.style.padding = '20px 50px';
        header.style.boxShadow = 'none';
    }
});
</script>
</body>
</html>